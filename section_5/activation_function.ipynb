{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "activation_function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zumi-n/Arcoiris/blob/master/section_5/activation_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7VsGM51NR43"
      },
      "source": [
        "# 活性化関数\n",
        "活性化関数は言わばニューロンを興奮させるための関数です。  \n",
        "ニューロンへの入力と重みをかけたものの総和にバイアス足し合わせた統合された値を、ニューロンの興奮状態を表す信号に変換します。  \n",
        "活性化関数が無いとニューロンの演算は単なる積の総和になってしまい、ニューラルネットワークから複雑な表現をする力が失われてしまいます。  \n",
        "活性化関数には様々な種類がありますが、 ここでは代表的なものをいくつかを紹介します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ho3gT_NR45"
      },
      "source": [
        "## ●ステップ関数\n",
        "ステップ関数は、階段状の関数です。  \n",
        "関数への入力$x$が0以下の場合は出力$y$が0、$x$が0より大きい場合は$y$は1になります。  \n",
        "これを式で表すと次のようになります。  \n",
        "\n",
        "$$\n",
        "y = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "0 & (x \\leqq 0)\\\\\n",
        "1 & (x > 0)\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$ \n",
        "\n",
        "ステップ関数は、以下のようなコードで実装することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCZn_bQONR46"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def step_function(x):\n",
        "    return np.where(x<=0, 0, 1)\n",
        "\n",
        "x = np.linspace(-5, 5)\n",
        "y = step_function(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmz-A5O3NR49"
      },
      "source": [
        "ステップ関数を用いると、ニューロンの興奮状態を0か1でシンプルに表現することができます。  \n",
        "実装が簡単である一方、$0$と$1$の中間の状態を表現できないというデメリットもあります。  \n",
        "ステップ関数は、ニューラルネットワークの起源であるパーセプトロンで用いられてきました。パーセプトロンはニューラルネットワークの一種と考えることもできますが、全ての信号が0か1で表されるよりシンプルなネットワークです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgqZUFvyNR4-"
      },
      "source": [
        "## ●シグモイド関数\n",
        "シグモイド関数は、0と1の間をなめらかに変化する関数です。  \n",
        "関数への入力$x$が小さくなると関数の出力$y$は0に近づき、$x$が大きくなるとyは1に近づきます。  \n",
        "シグモイド関数は、ネイピア数の累乗を表す$\\exp$を用いて次の式のように表します。\n",
        "\n",
        "$$y = \\frac{1}{1+\\exp(-x)}$$ \n",
        "\n",
        "この式において、$x$の値が負になり0から離れると、分母が大きくなるため$y$は0に近づきます。  \n",
        "また、$x$の値が正になり0から離れると、$\\exp(-x)$は0に近づくため$y$は1に近づきます。式からグラフの形状を想像することができますね。  \n",
        "\n",
        "シグモイド関数は、以下のようなコードで実装することができます"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTihVwmqNR4-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def sigmoid_function(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "x = np.linspace(-5, 5)\n",
        "y = sigmoid_function(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsaO6O8qNR5B"
      },
      "source": [
        "シグモイド関数はステップ関数と比べて滑らかであり、0と1の中間を表現できます。  \n",
        "また、この関数のメリットの1つに、微分が扱いやすいという特性があります。  \n",
        "シグモイド関数はニューラルネットワークで古くからよく使用されてきました。  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oERpQdHpNR5B"
      },
      "source": [
        "## ●tanh\n",
        "tanhはハイパボリックタンジェント（hyperbolic tangent）と読みます。  \n",
        "tanhは-1と1の間をなめらかに変化する関数です。\n",
        "曲線の形状はシグモイド関数に似ていますが、0を中心とした対称になっているのでバランスのいい活性化関数です。  \n",
        "tanhは、シグモイド関数と同じくネイピア数の累乗を用いた式で表されます。  \n",
        "\n",
        "$$y = \\frac{\\exp(x)-\\exp(-x)}{\\exp(x)+\\exp(-x)}$$ \n",
        "\n",
        "tanhは、以下のようなコードで実装することができます。  \n",
        "Numpyのtanh関数を用いれば、tanhを簡単に利用することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOilt5I1NR5C"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def tanh_function(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "x = np.linspace(-5, 5)\n",
        "y = tanh_function(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOrn1-23NR5E"
      },
      "source": [
        "## ●ReLU\n",
        "ReLUはランプ関数とも呼ばれ、$x>0$の範囲でのみ立ち上がるのが特徴的な活性化関数です。  \n",
        "ReLUは、次の式で表されます。\n",
        "\n",
        "$$\n",
        "y = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "0 & (x \\leqq 0)\\\\\n",
        "x & (x > 0)\n",
        "\\end{array}\n",
        "\\right.\n",
        "$$ \n",
        "\n",
        "関数への入力$x$が負の場合、関数の出力は$y$は0に、$x$が正の場合、$y$は$x$と等しくなります。\n",
        "\n",
        "ReLUは、以下のようなコードで実装することができます。  \n",
        "Numpyのwhere関数を使用していますが、これはx <= 0のときは0に、この条件を満たしていないときはxになることを意味します。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJvbHO2PNR5F"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def relu_function(x):\n",
        "    return np.where(x <= 0, 0, x)\n",
        "\n",
        "x = np.linspace(-5, 5)\n",
        "y = relu_function(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KLBfVkHNR5H"
      },
      "source": [
        "シンプルであり、なおかつ層の数が多くなっても安定した学習ができるので、最近のディープラーニングでは主にこのReLUが出力層以外の活性化関数として用いられています。 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5Sba83nNR5I"
      },
      "source": [
        "## ●恒等関数\n",
        "恒等関数は、入力をそのまま出力として返す関数です。  \n",
        "恒等関数の形状は直線になります。  \n",
        "恒等関数は、次のシンプルな式で表されます。  \n",
        "\n",
        "$$ y = x $$ \n",
        "\n",
        "また、恒等関数は以下のようなコードで実装することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSPKiSCJNR5I"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "x = np.linspace(-5, 5)\n",
        "y = x\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIoIVcN1NR5K"
      },
      "source": [
        "ニューラルネットワークの出力層では、この恒等関数と、後述するソフトマックス関数が活性化関数としてよく使用されます。 \n",
        "\n",
        "恒等関数は、以前に解説した回帰問題を扱う際によく選択されます。  \n",
        "出力の範囲に制限が無く連続的なため、連続的な数値を予測するのに適しているからです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYC-tJmuNR5L"
      },
      "source": [
        "## ●ソフトマックス関数\n",
        "ソフトマックス関数は、以前に解説した分類問題を扱うのに適した活性化関数で、他の活性化関数と比べて少々トリッキーな数式で表します。    \n",
        "活性化関数の出力を$y$、入力を$x$とし、同じ層のニューロンの数をnとするとソフトマックス関数は次の式で表されます。\n",
        "\n",
        "（式 1）\n",
        "$$ y = \\frac{\\exp(x)}{\\sum\\limits_{k=1}^n\\exp(x_k)} $$\n",
        "\n",
        "この式で、右辺の分母$\\sum\\limits_{k=1}^n\\exp(x_k)$は同じ層の各活性化関数への入力$x_k$から$\\exp(x_k)$を計算し足し合わせたものです。  \n",
        "また、次の関係で表されるように、同じ層の全ての活性化関数の出力を足し合わせると1になります。\n",
        "\n",
        "$$ \\sum\\limits_{l=1}^n(\\frac{\\exp(x_l)}{\\sum\\limits_{k=1}^n\\exp(x_k)})=\\frac{\\sum\\limits_{l=1}^n\\exp(x_l)}{\\sum\\limits_{k=1}^n\\exp(x_k)}=1 $$ \n",
        "\n",
        "これに加えて、指数関数には常に0より大きいという特性があるので、$0<y<1$となります。  \n",
        "このため、（式 1）のソフトマックス関数は、ニューロンが対応する枠に分類される確率を表現することができます。 \n",
        "\n",
        "ソフトマックス関数は、以下のコードで実装することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_kuVYpJNR5L"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax_function(x):\n",
        "    return np.exp(x)/np.sum(np.exp(x)) # ソフトマックス関数"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T30IusjANR5O"
      },
      "source": [
        "ソフトマックス関数の分母、$\\sum\\limits_{k=1}^n\\exp(x_k)$はNumpyのsum関数を用いて、`np.sum(np.exp(x))`と記述します。\n",
        "\n",
        "このコードの`softmax_function`関数を実行してみましょう。  \n",
        "Numpyの適当な配列を入力し、出力を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMcstvpNNR5O"
      },
      "source": [
        "y = softmax_function(np.array([1,2,3]))\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DAX3REyNR5Q"
      },
      "source": [
        "出力された全ての要素は0から1の範囲に収まっており、合計は1となっています。  \n",
        "ソフトマックス関数が機能していることが確認できますね。\n",
        "\n",
        "以上のような様々な活性化関数を、層のタイプや扱う問題によって使い分けることになります。"
      ]
    }
  ]
}